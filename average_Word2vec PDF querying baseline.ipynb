{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zXYKxWNJ0SAg",
    "outputId": "8af36df2-0f28-4813-ba62-f4e6f51cbc66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tT2L-G4I0z05",
    "outputId": "7ce2e267-e24c-445e-f9a6-092b46cc692c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "! pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "TwSVEuE9CIpW"
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OAYiltHgDXSu",
    "outputId": "d8ebebd8-8fd0-49dd-c4ae-c37c3aa7ad93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "219j7liF4lxS"
   },
   "source": [
    "Finding analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "VsNGRfAvJoxg"
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_file):\n",
    "\n",
    "    \"\"\"Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_file: The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings, each representing a page of the PDF.\n",
    "    \"\"\"\n",
    "\n",
    "    text = []\n",
    "    with open(pdf_file, 'rb') as pdf_reader:\n",
    "        reader = PyPDF2.PdfReader(pdf_reader)\n",
    "        for page in reader.pages:\n",
    "            text.append(page.extract_text())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7cvFo6DuONIE",
    "outputId": "3b02cadf-a58e-4028-c42a-0cb69976980d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def chunk_text(text, chunk_size=300):\n",
    "    \"\"\"Chunks a text into smaller segments based on sentence boundaries.\n",
    "\n",
    "    Args:\n",
    "        text: The text to chunk.\n",
    "        chunk_size: The desired maximum size of each chunk.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings, each representing a chunk of the text.\n",
    "    \"\"\"\n",
    "\n",
    "   # Join the list of strings into a single string\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_size = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        num_words = len(sentence.split())\n",
    "        if current_chunk_size + num_words <= chunk_size:\n",
    "            current_chunk.append(sentence)\n",
    "            current_chunk_size += num_words\n",
    "        else:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_chunk_size = num_words\n",
    "\n",
    "    # Add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "PQTxjHgGPiU6"
   },
   "outputs": [],
   "source": [
    "def calculate_average_vector(text, wv):\n",
    "    \"\"\"Calculates the average Word2Vec vector for a text.\n",
    "\n",
    "    Args:\n",
    "        text: The text to process.\n",
    "        wv: The Word2Vec model.\n",
    "\n",
    "    Returns:\n",
    "        The average Word2Vec vector for the text.\n",
    "    \"\"\"\n",
    "\n",
    "    sent_vectors = []\n",
    "    for sentence in text.split('.'):\n",
    "        sent_vector = np.zeros((300,))\n",
    "        count = 0\n",
    "        for word in sentence.split():\n",
    "            if word in wv:\n",
    "                count += 1\n",
    "                sent_vector += wv[word]\n",
    "        if count > 0:\n",
    "            sent_vectors.append(sent_vector / count)\n",
    "\n",
    "    if sent_vectors:\n",
    "        return sum(sent_vectors) / len(sent_vectors)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "bpno_-rHTlnG"
   },
   "outputs": [],
   "source": [
    "\n",
    "def find_similar_paragraphs(query, chunk_vectors, top_n=3):\n",
    "    \"\"\"Finds the most similar paragraphs to a query.\n",
    "\n",
    "    Args:\n",
    "        query: The query text.\n",
    "        paragraph_vectors: A list of average Word2Vec vectors representing the paragraphs.\n",
    "        top_n: The number of most similar paragraphs to return.\n",
    "\n",
    "    Returns:\n",
    "        A list of the most similar paragraphs.\n",
    "    \"\"\"\n",
    "\n",
    "    query_vector = calculate_average_vector(query, wv)\n",
    "    similarities = cosine_similarity([query_vector], chunk_vectors)[0]\n",
    "    top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "    return [chunks[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "FLviZYnmdiz3"
   },
   "outputs": [],
   "source": [
    "# Load the PDF file and extract the text\n",
    "\n",
    "pdf_file = '/content/sample_data/Improvingaccuracyofpretrainedwordembeddingsforsentimentanalysis.pdf'\n",
    "text = extract_text_from_pdf(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V_dtgROW2VET",
    "outputId": "9f383eab-bfa3-4a4b-86ac-087f8b4ce960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Improving the Accuracy of Pre-trained Word Embeddings for \n",
      "Sentiment Analysis \n",
      " \n",
      "Seyed Mahdi Rezaeinia a,b, Ali Ghodsi  a, Rouhollah Rahmani  b \n",
      "a Department of Statistics and Actuarial Science, University of Waterloo , Waterloo,  Canada   \n",
      "b Network Science and Technology Department, University of Tehran , Tehran, Iran  \n",
      " \n",
      " \n",
      "Abstract  \n",
      "Sentiment analysis is one of the well -known tasks and fast growing research areas in natural language \n",
      "processing (NLP) and text classifications. This technique has become an essential part of a wide range of \n",
      "applications  including politics,  business , advertising and marketing.  There are various techniques  for \n",
      "sentiment analysis, but r ecently  word embeddings methods  have  been widely used in sentiment \n",
      "classification tasks.  Word2Vec and GloV e are currently among the most accurate and usable word \n",
      "embedding methods which can convert words into meaningful vectors . However, these methods ignore \n",
      "sentiment information of texts  and need  a huge corpus of texts for training and generating exact vectors  \n",
      "which are used as inputs of deep learning models.  As a result, because of the smal l size of some corpuses, \n",
      "researcher often have to use pre- trained word embeddings which were trained on other large text corpus  \n",
      "such as Google News with about 100 billion  words.  The increasing accuracy of pre- trained word \n",
      "embeddings has a great  impact on sentiment analysis research.  In this paper we propose a novel method , \n",
      "Improved Word Vectors (IWV),  which increases the accuracy of pre- trained word embeddings in sentiment \n",
      "analysis.  Our method is based on Part-of-Speech (POS) tagging techniques, lexicon -based approaches \n",
      "and Word2Vec/GloV e methods. We tested the accuracy of our method via different deep learning models  \n",
      "and sentiment datasets.  Our experiment results show  that Improved Word Vectors (IWV)  are very effect ive \n",
      "for sentiment analysis . \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "1. Introduction \n",
      "Sentiment analysis is a practical technique that allows businesses, researchers, governments, politicians \n",
      "and organizations to know about people’s sentiments, which play an important role in decision making \n",
      "processes.  Word Embedding is one of the most useful deep learning methods  used for constructing  vector \n",
      "representations of words and documents.  These methods have received a lot of attention in text and \n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3oWKjJx_BZuf",
    "outputId": "6a0a8420-df97-4eec-e16a-5564dd37afd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2934"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ASzCXYf3fFH",
    "outputId": "ada9eb92-3616-46e8-9974-cdc8746713eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4894\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'text' is a list of strings where each string represents a sentence or paragraph\n",
    "# and you want to count the total number of words in all strings within the list:\n",
    "total_words = sum(len(sentence.split()) for sentence in text)\n",
    "\n",
    "# Print the total number of words\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "VFxDfM2k2bjN"
   },
   "outputs": [],
   "source": [
    "# Chunk the text into smaller segments\n",
    "\n",
    "chunks = chunk_text(text , 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H9qcUnpo2wHf",
    "outputId": "0e27d8cb-a55a-4140-bb75-4f91107705c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "QougK4wf3Vin"
   },
   "outputs": [],
   "source": [
    "# Calculate average Word2Vec vectors for each chunk\n",
    "\n",
    "chunk_vectors = [calculate_average_vector(chunk, wv) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vvRlz_VQ7Qu-",
    "outputId": "bd62461b-45ac-493f-bb40-cb84da34023a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "[ 0.02500691  0.03336662  0.04037175  0.08060622 -0.07895257  0.01929872\n",
      "  0.00316947 -0.07053979  0.07782849  0.05283274 -0.03058976 -0.09124789\n",
      "  0.02250694  0.04465533 -0.09763794  0.0581762   0.01454134  0.06499389\n",
      " -0.00192077 -0.08878534 -0.03924489  0.02250768 -0.02983831  0.03267289\n",
      " -0.01264794 -0.03057744 -0.11700708  0.03886558  0.00092304 -0.00326418\n",
      " -0.03715216 -0.04347403 -0.00144736 -0.02838854  0.01728254  0.0469247\n",
      " -0.05205797  0.04318216  0.09192727  0.00567166  0.08865903  0.00851114\n",
      "  0.00708952  0.06012082  0.0024702  -0.04440909 -0.06786914 -0.00424986\n",
      " -0.01541388  0.05263813 -0.03905487  0.03958233 -0.05106843 -0.06978613\n",
      "  0.00178676  0.07512456 -0.04147037 -0.08982822  0.028208   -0.06954416\n",
      " -0.04710114  0.01040004 -0.09551124 -0.07041034 -0.04339964  0.0108907\n",
      " -0.02542109  0.1044823   0.00659308  0.0269096  -0.00834378 -0.02253295\n",
      "  0.09255501  0.01054245 -0.02575177 -0.06439972  0.08203788  0.04434809\n",
      "  0.03397023  0.08976536  0.00273166 -0.05336929  0.07415366  0.01979138\n",
      "  0.03853549 -0.05939941 -0.08165697  0.12061828  0.01506593  0.03772059\n",
      "  0.05130443 -0.02278288 -0.06413317 -0.04487271  0.00696361  0.00184993\n",
      "  0.00606594  0.00074738  0.06654174  0.01778687  0.03258429 -0.05476568\n",
      " -0.0161293  -0.00272779 -0.00358881 -0.00057871 -0.08102084 -0.03029447\n",
      "  0.05022688 -0.08521892 -0.02829757  0.00701537 -0.01647811  0.01201469\n",
      "  0.08485621  0.04287688  0.02418549 -0.03334172  0.10056254  0.03413692\n",
      " -0.0860556   0.01340591 -0.06316444  0.04368537  0.02950543 -0.00125197\n",
      " -0.05119461 -0.00179376 -0.00104485  0.05758312 -0.00750326 -0.0664057\n",
      " -0.06705739 -0.03401682 -0.01279581 -0.04979361  0.05614752 -0.03284338\n",
      "  0.00333129  0.06397367  0.06211965 -0.00251313  0.00245339  0.02346294\n",
      " -0.00215691 -0.04978757  0.02609036  0.00787489 -0.03111694 -0.01310075\n",
      "  0.05992105 -0.01546744 -0.051841    0.0321905  -0.0072121  -0.05584964\n",
      " -0.07662283 -0.02389662 -0.06034713 -0.02903524 -0.01414855  0.075903\n",
      "  0.02565482  0.01248765 -0.01555883 -0.00721046  0.03385922 -0.0004236\n",
      " -0.00945747 -0.03425779 -0.04394214 -0.0273412  -0.07724679 -0.06747631\n",
      "  0.03293171  0.00853131  0.06296593 -0.12350509 -0.04122669 -0.03595933\n",
      " -0.07821218 -0.07208693  0.00685273  0.03134918  0.00855453 -0.0360666\n",
      " -0.04644222  0.0470112   0.08245276  0.07764477 -0.02875539 -0.02881901\n",
      "  0.01287461 -0.07253468 -0.1188189  -0.01513759 -0.00419996 -0.04179422\n",
      " -0.01269658 -0.11069779  0.00103787  0.04052501 -0.0372651   0.00946882\n",
      " -0.05853898  0.00487202  0.00356634  0.01622823  0.00940976 -0.02787365\n",
      " -0.00870687  0.01626958 -0.04792037  0.05408626 -0.07087459  0.04061793\n",
      "  0.12306871  0.03120401 -0.03770274 -0.00400849 -0.02482108  0.00944161\n",
      " -0.01581826 -0.06443799  0.00476462 -0.03837382  0.0003464   0.00512577\n",
      "  0.03535818 -0.01799248 -0.00863414 -0.0359716   0.06999479  0.04655785\n",
      " -0.01931388 -0.04503159 -0.02042716 -0.06292193  0.0563426   0.00317446\n",
      "  0.01676855  0.02900324  0.03713799 -0.04375298 -0.00477717 -0.01122147\n",
      " -0.01174256  0.08904763 -0.00800857 -0.03387545  0.00183486  0.01863476\n",
      " -0.02066591  0.04211628  0.03308105 -0.0496521   0.00513866 -0.0159522\n",
      " -0.036734   -0.05854854  0.05267807 -0.025844   -0.04700729  0.03158596\n",
      " -0.00486867  0.07831134 -0.01232866 -0.05172602 -0.04672216 -0.02131681\n",
      "  0.02082526  0.0788726   0.01519062  0.00136223  0.04768754 -0.02722457\n",
      " -0.07258586 -0.05775949  0.02319682  0.01371907 -0.00761579  0.0523163\n",
      "  0.04545981  0.03003838  0.04619172 -0.0110872  -0.03734684  0.02757192\n",
      "  0.02344264  0.05758998 -0.07554406  0.00975155 -0.06731476  0.0183493\n",
      " -0.00504708 -0.01095117  0.00445064 -0.03886336  0.00255281  0.01276122]\n"
     ]
    }
   ],
   "source": [
    "print(len(chunk_vectors))\n",
    "print(chunk_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "F4JRifLx7bgH"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "\n",
    "query = \" accuracy of the Word2vec and Glove ?\"\n",
    "similar_chunks = find_similar_paragraphs(query, chunk_vectors, top_n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PnULbEr97zel",
    "outputId": "ca922ad4-8a0d-4098-952e-043ea9660b35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similar_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "Y_pkEEBi856v",
    "outputId": "3cc3fc9c-5cda-497c-f659-dd97657ae11a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[15] have used pre- trained Word2Vec as  a word embedding representation for recommending Idioms \\nin essay w riting . As a result, i ncreasing the accuracy of pre- trained word embedding is very important and plays a vital role \\nin sentiment classification methods. Zhang and Wallace  [10] combined pre- trained Word2Vec and GloVe \\nvectors in their deep learning model, but the accuracies were  decreased. Some results are shown in       \\ntable 1. Table1: The results of the combination of pre-trained Word2Vec and Glove on three datasets  [10] \\nDataset  Reduced accuracy rate (%) \\nMovie reviews ( MR) -0.22 %  \\nStanford sentiment treebank  (SST)  -1.1%  \\nTREC  -0.17%  \\n \\n \\nAccording to table1, combination of pre- trained Word2Vec and Glove decreased the ac curacy  of text and \\nsentiment classification on some datasets. Also, Kamkarhaghighi  and Makrehchi  [16] have proposed an \\nalgorithm for increasing the accuracy of pre- trained Word2Vec and Glove. Their algorithm was tested on \\ntwo datasets and the accuracy of Word2V ec was decreased on one dataset by the proposed algorithm. In \\nthe following section we present in detail our proposed model, its algorithm and the proposed deep learning \\nmodel for checking our  method. 3. Proposed method \\nIn our proposed method, Improved Word Vector  (IWV) we have increase d the accuracy of word embedding \\nvectors  based on the combination of natural language processing techniques, lexicon -based approaches \\nand W ord2Vec/GloVe methods which have high accuracies. The main architecture of the proposed  method \\nhas been shown in f igure 1. Figure1: The main architecture of the proposed method (Improved word vector)  \\n \\n3.1. Word2Vec and GloVe \\nWord2Vec and GloVe are two successful word embedding algorithms which have high accuracy. Word2Vec \\nis based on c ontinuous Bag -of-Words  (CBOW) and Skip- gram architectures  which can provide high quality \\nword embedding vectors.'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "SkLjRyFf9vYs",
    "outputId": "023832b4-f74e-47a1-9f61-84090129b052"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Figure  4: The accuracy  (%) comparisons of three methods on four deep learning models for CR dataset  \\n \\n 7777.57878.57979.580\\nModel 1 Model 2 Model 3 Model 4The MR dataset\\nGlove Word2Vec   IWV\\n7979.58080.58181.58282.58383.5\\nModel 1 Model 2 Model 3 Model 4The CR dataset\\nGlove Word2Vec   IWV\\nAccuracy (%)  \\n Accuracy (%)   Figure 4 indicates that the IWV method generally has performed better than other per -trained word \\nembeddings for sentiment analysis of CR dataset. The CR is an unbalanced dataset w hich contains 2397 \\npositive and 1406 negative reviews. As can be seen, pre- trained Word2vec embedding is almost more \\naccurate than pre- trained Glove embedding, however it is reverse in the model 2. The IWV provides  \\nabsolute accuracy improvement s of 0.7%, 0.4%, 1.1% and 0.2% for model 1, model 2, model 3 and model \\n4, respectively . 5. Conclusion  \\n \\nIn this paper, we proposed a new method to improve the accuracy of well -known pre- trained word \\nembeddings  for sentiment analysis . Our method has improved the accuracy of pre- trained word \\nembeddings based on the combination of three approaches  such as lexicon- based approach, POS tagging \\napproach and Word2Vec/GloVe approach. In order to ensure about the accuracy of our  proposed method,  \\nwe have tested it  nine times  with different deep learning models and sentiment  datasets. The experimental \\nresults indicated  that our method has increased the accuracy of sentiment classification tasks i n all models \\nand datasets . Briefly, the main advantages of  the proposed method are:  \\n \\n• Because of the accuracy of pre- trained Word2Vec/Glove, adding any vector to them decreased the \\naccuracy  according to previous researches , but our  proposed method has increased the accuracy \\nof pre- trained vectors  in sentiment analysis for the first time.'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_chunks[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdS4mqIGFQVB"
   },
   "source": [
    "Improvement of the solution by doing optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXCikhtfIK_h"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
